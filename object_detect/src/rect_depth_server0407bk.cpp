#include <ros/ros.h>
#include <image_transport/image_transport.h>
#include <cv_bridge/cv_bridge.h>
#include <sensor_msgs/image_encodings.h>
#include <opencv2/imgproc/imgproc.hpp>
#include <opencv2/highgui/highgui.hpp>
#include <opencv2/opencv.hpp>
#include <opencv2/core/core.hpp>                       //zliu7
#include <opencv2/video/background_segm.hpp>           //zliu7
#include "geometry_msgs/Pose2D.h"
#include <std_msgs/Time.h>
#include <iostream>
#include <stdio.h>
#include <stdlib.h>
#include <geometry_msgs/Twist.h>
#include <vector>
#include <algorithm>
#include <math.h>
#include <object_detect/poseDepth.h>
#include <tf2/LinearMath/Quaternion.h>
#include <geometry_msgs/Quaternion.h>
#include <geometry_msgs/Pose.h>
#include <tf2/LinearMath/Matrix3x3.h>
#include <pcl/point_types.h>
#include <sensor_msgs/Image.h>
#include <sensor_msgs/PointCloud2.h>
#include <pcl_conversions/pcl_conversions.h>
#include <pcl/point_cloud.h>

using namespace pcl;
using namespace pcl::io;
using namespace cv;
using namespace std;
using namespace cv;	
using namespace std;
using namespace tf2;
using namespace Eigen;																	//=T
static const std::string OPENCV_WINDOW = "Image window";
const float PI=3.1415926;
Matrix<float,4,4> mat_kinect;
pcl::PointCloud<pcl::PointXYZ> cloud;
bool receive = false;
ros::Subscriber object_pcl;

ros::Publisher object_center_pub;   //zxx
ros::Publisher object_center_2d_pub;   //zxx
vector<Point2f> center2ds;
Point2f center2d;


//zxx
void  cloud_cb (const sensor_msgs::PointCloud2ConstPtr& input)
{
//pcl::PointCloud<pcl::PointXYZ> cloud; 
  sensor_msgs::Image image_;
  pcl::fromROSMsg (*input, cloud);
  receive = true;


//  cloud.points.at<float>(256,64);369.90
}
const tf2::Quaternion midq(0.5,0.5,0.5,-0.5);//x,y,z,w
const tf2::Matrix3x3 MID_MATRIX(midq);
geometry_msgs::Quaternion gq;
int euler2matrix2quaternion(double x,double y,double z)
{
	tf2::Matrix3x3 eulerMatrix;
	x = -x;
	y = -y;
	eulerMatrix.setRPY(x,y,z);
		
	eulerMatrix = eulerMatrix * MID_MATRIX ;
	tf2::Quaternion tq;

	eulerMatrix.getRotation(tq);
	ROS_INFO("eulQ:    X=%f,Y=%f,Z=%f,W=%f",tq.x(),tq.y(),tq.z(),tq.w());
	gq.x = tq.x();
	gq.y = tq.y();
	gq.z = tq.z();
	gq.w = tq.w();
	return 0;
}
float geuler=0,gangel=0;
const float CONST  = 1.13491;//-0.84;

//======================================================================================================================================================
//======================================================================================================================================================
geometry_msgs::Pose2D position;				// create a struct to publish cordinate info later on
ros::Publisher pub;
////////////////definition of background substraction//////////////

 Mat fgMaskMOG; //fg mask generated by MOG method
 Ptr <BackgroundSubtractor> pMOG; //MOG Background subtractor

////////////////////////////////////////////////////////////////////
int iLowH_1 = 0;
int iHighH_1 = 69;
int iLowS_1 = 43;
int iHighS_1 = 140;
int iLowV_1 = 126;
int iHighV_1 = 237;


//default capture width and height
const int FRAME_WIDTH = 1288;
const int FRAME_HEIGHT = 964;
//max number of objects to be detected in frame
const int MAX_NUM_OBJECTS=50;
//minimum and maximum object area
const int MIN_OBJECT_AREA = 2*2;
const int MAX_OBJECT_AREA = FRAME_HEIGHT*FRAME_WIDTH/1.5;
//names that will appear at the top of each window
const string windowName = "Original Image";
const string windowName2 = "Thresholded Image 1";

const string windowName_g = "Gaussian Image 1";

const string trackbarWindowName = "Trackbars";

Mat DistortedImg;											//storage for copy of the image raw
Mat	UndistortedImg;											//


double cameraM[3][3] = {{ 521.906925, 0, 330.571743}, {0, 522.912033, 267.759287}, {0, 0, 1}} ;                            
Mat cameraMatrix = Mat(3, 3, CV_64FC1, cameraM);//.inv();

double distortionC[5] = {0.160792,-0.279956,-0.000324,-0.000310, 0};				//distortioncoefficient to be edited
Mat distCoeffs = Mat(1, 5, CV_64FC1, distortionC);							

double rArray[3][3] = {{1, 0, 0}, {0, 1, 0}, {0, 0, 1}};
Mat RArray = Mat(3, 3, CV_64FC1, rArray);					//originally CV_64F

double newCameraM[3][3] = {{532.206543, 0, 330.202770}, {0, 533.930420, 267.077297}, {0, 0, 1}};
Mat NewCameraMatrix = Mat(3, 3, CV_64FC1, newCameraM);
Size UndistortedSize(640, 360);

Mat map1;
Mat map2;												

int xa,ya;
int xa_old, ya_old;
int x_offset=20;               
int y_offset=0;                 


double x_init, z_init, x_final, z_final;
double alpha, beta, angle_dif;

double distance_x, distance_z, rotation_y, asymptote;
int n = 0;
int t= 0;


void createTrackbars(){
	//create window for trackbars

    namedWindow("Control_1", CV_WINDOW_AUTOSIZE);
	//create memory to store trackbar name on window
	//create trackbars and insert them into window
	//3 parameters are: the address of the variable that is changing when the trackbar is moved(eg.H_LOW),
	//the max value the trackbar can move (eg. H_HIGH), 
	//and the function that is called whenever the trackbar is moved(eg. on_trackbar)
	//                                  ---->    ---->     ---->      
 //Create trackbars in "Control" window
	cvCreateTrackbar("LowH_1", "Control_1", &iLowH_1, 179); //Hue (0 - 179)
	cvCreateTrackbar("HighH_1", "Control_1", &iHighH_1, 179);

	cvCreateTrackbar("LowS_1", "Control_1", &iLowS_1, 255); //Saturation (0 - 255)
	cvCreateTrackbar("HighS_1", "Control_1", &iHighS_1, 255);

	cvCreateTrackbar("LowV_1", "Control_1", &iLowV_1, 255); //Value (0 - 255)
	cvCreateTrackbar("HighV_1", "Control_1", &iHighV_1, 255);
	
}
string intToString(int number)
{
	std::stringstream ss;
	ss << number;
	return ss.str();
}


void drawObject(int x, int y,Mat &frame){  //RED TARGET

	//use some of the openCV drawing functions to draw crosshairs
	//on the tracked image!

	circle(frame,Point(x,y),20,Scalar(0,255,0),2);

    if(y-25>0)
    line(frame,Point(x,y),Point(x,y-25),Scalar(0,255,0),2);
    else line(frame,Point(x,y),Point(x,0),Scalar(0,255,0),2);
    if(y+25<FRAME_HEIGHT)
    line(frame,Point(x,y),Point(x,y+25),Scalar(0,255,0),2);
    else line(frame,Point(x,y),Point(x,FRAME_HEIGHT),Scalar(0,255,0),2);
    if(x-25>0)
    line(frame,Point(x,y),Point(x-25,y),Scalar(0,255,0),2);
    else line(frame,Point(x,y),Point(0,y),Scalar(0,255,0),2);
    if(x+25<FRAME_WIDTH)
    line(frame,Point(x,y),Point(x+25,y),Scalar(0,255,0),2);
    else line(frame,Point(x,y),Point(FRAME_WIDTH,y),Scalar(0,255,0),2);

	putText(frame,intToString(x)+","+intToString(y),Point(x,y+30),1,1,Scalar(0,255,0),2);

}

void morphOps(Mat &thresh)
{
	//create structuring element that will be used to "dilate" and "erode" image.
    //dilate with larger element so make sure object is nicely visible

	erode(thresh,thresh,getStructuringElement( MORPH_RECT,Size(2,2)));
	dilate(thresh,thresh,getStructuringElement( MORPH_RECT,Size(2,2)));	
}

bool ojbtarget = false;

void trackFilteredObject1(int &x, int &y, Mat threshold, Mat &cameraFeed, int &x_offset, int &y_offset)
{
	ROS_INFO("trackFilteredObject1");

	Mat temp;
	threshold.copyTo(temp);				//these two vectors needed for output of findContours
	
	vector< vector<Point> > contours;
	vector<Vec4i> hierarchy;			//find contours of filtered image using openCV findContours function
	
	findContours(temp,contours,hierarchy,CV_RETR_EXTERNAL,CV_CHAIN_APPROX_SIMPLE );
	
	double refArea = 0;
	bool objectFound = false;
	ojbtarget = false;
	if (hierarchy.size() > 0) 			//if number of objects greater than MAX_NUM_OBJECTS we have a noisy filter
	{
		int numObjects = hierarchy.size();
		
		if(numObjects<MAX_NUM_OBJECTS)
		{
			for (int index = 0; index >= 0; index = hierarchy[index][0]) 
			{
				Moments moment = moments((cv::Mat)contours[index]);	//use moments method to find our filtered object
				double area = moment.m00;

				//if the area is less than 20 px by 20px then it is probably just noise
				//if the area is the same as the 3/2 of the image size, probably just a bad filter
				//we only want the object with the largest area so we safe a reference area each
				//iteration and compare it to the area in the next iteration.
				//if(area>MIN_OBJECT_AREA && area<MAX_OBJECT_AREA && area>refArea)
				if(area>MIN_OBJECT_AREA && area>refArea)
				{
					x = moment.m10/area;
					y = moment.m01/area;
					ojbtarget=true;
					ROS_INFO(" IMAGE DETECTED!");
					refArea = area;
			
					if (n == 0)
					{
					xa_old = x;
					ya_old = y;
					n = 1;
					}
			
					if ((abs(xa_old-x)<= 2) && (abs(ya_old-y) <= 2))
					{
						xa = xa_old;
						ya = ya_old;
					}
					else 
					{	
					    xa = x+x_offset;
					    ya = y+y_offset;
					    xa_old = x+x_offset;
					    ya_old = y+y_offset;				    
				        }
				}
				else {ROS_INFO("no thing!");ojbtarget=false;}

			}
			//let user know you found an object
			//if(objectFound == true)
			//{
			//putText(cameraFeed,"Tracking Object",Point(0,50),2,1,Scalar(0,255,0),2);
				//draw object location on screen
			//drawObject(xa,ya,cameraFeed);

		}

		else //putText(cameraFeed,"TOO MUCH NOISE! ADJUST FILTER",Point(0,50),1,2,Scalar(0,0,255),2);
		{ROS_INFO("no thing!");ojbtarget=false;}
	}
ROS_INFO("obj detected?%d",ojbtarget);
}


//======================================================================================================================================================

geometry_msgs::Pose srvpose;
bool srvsend;
int gwidth;
bool setsrvPose()
{
	srvpose.position.z = cloud.points[center2d.y*cloud.width+center2d.x].z;
	srvpose.position.x = cloud.points[center2d.y*cloud.width+center2d.x].x;
	srvpose.position.y = -cloud.points[center2d.y*cloud.width+center2d.x].y;
        
	if(isnan(srvpose.position.x) || isnan(srvpose.position.y) || isnan(srvpose.position.z))
		return false;
	if(srvpose.position.z>1.8 ||srvpose.position.z < 1.0 ) return false;
	ROS_INFO("DEPTH : %f,%f,%f",srvpose.position.x,srvpose.position.y,srvpose.position.z);
	

	Matrix<float,4,1> Pr;
	Matrix<float,4,1> Pc;
	Pc << 	srvpose.position.x*1000,
	  	srvpose.position.y*1000,
	  	srvpose.position.z*1000,
		    1;

	Pr = mat_kinect *Pc;

	srvpose.position.x = Pr(0,0)/1000;
	srvpose.position.y = Pr(1,0)/1000;
	srvpose.position.z = Pr(2,0)/1000;
	if (srvpose.position.z >-0.1 && srvpose.position.z < 0.4)
    		srvsend = true;
	ROS_ERROR("rotation:%f,%f,%f",srvpose.position.x,srvpose.position.y,srvpose.position.z);
	return true;
  
}
bool getsrvPose(object_detect::poseDepth::Request &req,object_detect::poseDepth::Response &res)
{
	if(req.flag && srvsend)
	{
		res.pose = srvpose;
		res.width = gwidth;
		srvsend = false;
		return true;
	}
	return false;
} 
  void imageCb(const sensor_msgs::ImageConstPtr& msg)								  //callback function defination
  {
     
	cv_bridge::CvImagePtr cv_ptr;													   
	try
	{
		cv_ptr = cv_bridge::toCvCopy(msg, sensor_msgs::image_encodings::BGR8);   			//convert ROS image to CV image and make copy of it storing in cv_ptr(a pointer)
	}
	catch (cv_bridge::Exception& e)
	{
		ROS_ERROR("cv_bridge exception: %s", e.what());
		return;
	}

	/*	image working procedure starting from here inside the main function.
	 *  The purpose of the image processing is to use the existing video to working out the 
	 *  cordinate of the detected object, using color extraction technique.
	 */
        cv::imwrite("rect.jpg",cv_ptr->image);
	bool trackObjects = true;
	bool useMorphOps = true;

	Mat cameraFeed;

	Mat HSV_1;

	Mat threshold_1;
	//x and y values for the location of the object
	int x=0, y=0;
	createTrackbars();

	//store image to matrix
	cv_ptr->image.copyTo(DistortedImg);											//=Tan= copy the image from ardrone to DistortedImg for processing
	initUndistortRectifyMap(cameraMatrix, distCoeffs, RArray, NewCameraMatrix, UndistortedSize, CV_32FC1, map1, map2);
	remap(DistortedImg, cameraFeed, map1, map2, INTER_LINEAR, BORDER_CONSTANT, Scalar(0,0,0));		// maybe try this one instead of the above two instead: cv::undistort(image, undistorted, cameraMatrix, distCoeffs);
	//////////////////////////////////////////background substraction 1////////////////////////////////////////////////////////////////
	pMOG->operator()(cameraFeed, fgMaskMOG);        

	Mat pFrame=fgMaskMOG;
	Mat element1 = getStructuringElement(0, Size(20,20), Point(-1,-1));
	morphologyEx(pFrame, pFrame, MORPH_CLOSE, element1);//闭运算

	vector<vector<Point> > contours;
	vector<Vec4i> hierarchy;			//find contours of filtered image using openCV findContours function



	findContours(pFrame, contours, hierarchy, CV_RETR_EXTERNAL, CV_CHAIN_APPROX_SIMPLE);  //ŒìË÷ÂÖÀª

	Mat result(pFrame.size(), CV_8U, Scalar(255));
	drawContours(result, contours, -1, Scalar(0), 2);// draw all contours//


	// Eliminate too short or too long contours  
	int cmin = 20;  // minimum contour length  //Change these two parameters
	int cmax = 500; // maximum contour length  
	vector<vector<Point> >::iterator itc = contours.begin();
	//vector<vector<Point> > contours;
	while (itc != contours.end())
	{

		if (itc->size() < cmin || itc->size() > cmax)
			itc= contours.erase(itc);
		else ++itc;
	}

	// Let's now draw black contours on white image  
	result.setTo(Scalar(255));
	drawContours(result, contours, -1, Scalar(0), 1);  // draw all contours £¬in black£¬with a thickness of 1    

	vector<vector<Point> >::const_iterator itcon = contours.begin();
	float angl;
	for (; itcon != contours.end(); ++itcon)
	{
		RotatedRect rRect = minAreaRect(Mat(*itcon));
		Point2f vertices[4];
		rRect.points(vertices);
		for (int i = 0; i < 4; ++i)
			line(result, vertices[i], vertices[(i + 1) % 4], CV_RGB(255, 0, 0), 2, 8, 0);//»­×îÐ¡Íâ°üŸØÐÎ
		ROS_INFO("width, height%d,%d",(int)rRect.size.width,(int)rRect.size.height);
		gwidth = (int)rRect.size.width<(int)rRect.size.height? (int)rRect.size.width:(int)rRect.size.height;
		//srvsend = false;
		if(rRect.size.width> rRect.size.height)
		{
		    Point2f pttA[2];
			pttA[0].x = (vertices[0].x + vertices[1].x) / 2;
			pttA[0].y = (vertices[0].y + vertices[1].y) / 2;
			pttA[1].x = (vertices[2].x + vertices[3].x) / 2;
			pttA[1].y = (vertices[2].y + vertices[3].y) / 2;
			Point2f pttCent;
			pttCent.x = (pttA[0].x + pttA[1].x) / 2;
			pttCent.y = (pttA[0].y + pttA[1].y) / 2;
  			//ROS_ERROR("pttCent%d,%d",(int)pttCent.x,(int)pttCent.y);
			center2d.x =(int) pttCent.x;
			center2d.y =(int) pttCent.y;
			srvsend = setsrvPose();
			ROS_INFO("CENTER2D , x,y%d,%d",(int)center2d.x,(int)center2d.y);
			line(result, pttA[0], pttA[1], CV_RGB(255, 0, 0), 2, 8, 0);
			circle(result, pttCent, 2, Scalar(0), 3);
		 	angl = atan2((pttA[1].y - pttA[0].y), (pttA[1].x - pttA[0].x));
	
			char buf[20];
	
			//putText(result, buf, rRect.center, CV_FONT_HERSHEY_COMPLEX, 1, Scalar(0, 0, 0));

		}
		else
		{
			Point2f ptt[2];
			ptt[0].x = (vertices[1].x + vertices[2].x) / 2;
			ptt[0].y = (vertices[1].y + vertices[2].y) / 2;
			ptt[1].x = (vertices[0].x + vertices[3].x) / 2;
			ptt[1].y = (vertices[0].y + vertices[3].y) / 2;
			Point2f pttCen;
			pttCen.x = (ptt[0].x + ptt[1].x) / 2;
			pttCen.y = (ptt[0].y + ptt[1].y) / 2;
			center2d.x =(int) pttCen.x;
			center2d.y =(int) pttCen.y;
			srvsend = setsrvPose();
			ROS_ERROR("CENTER2D , x,y%d,%d",(int)center2d.x,(int)center2d.y);
			line(result, ptt[0], ptt[1], CV_RGB(255, 0, 0), 2, 8, 0);
			circle(result, pttCen, 2, Scalar(0), 3);
			 angl = atan2((ptt[1].y - ptt[0].y), (ptt[1].x - ptt[0].x));

			char buf1[20];
	
			//putText(result, buf1, rRect.center, CV_FONT_HERSHEY_COMPLEX, 1, Scalar(0, 0, 0));
		}
		gangel = angl;
		ROS_INFO( "before angl:%f rad %f", gangel,angl * 180 / CV_PI);
		if (angl<0)
			angl=-angl;
		else angl = PI-angl;
		gangel = angl;
		ROS_INFO( "after angl:%f rad %f", gangel,angl * 180 / CV_PI);
		double euler=(double)(CONST+angl);

		euler2matrix2quaternion(PI,0,euler);
		ROS_INFO("euler:%f\n",euler);
		geuler=euler;
		
		imshow("6 Final Result", result);
		if(srvsend)
			{srvpose.orientation = gq;break;}
	}

	     
	cv::waitKey(3);  
	
  
}

int main(int argc, char** argv)
{
  ros::init(argc, argv, "object_rect");
  ros::NodeHandle nh_;
  image_transport::ImageTransport it_(nh_);
  image_transport::Subscriber image_sub_;
  /*
  mat_kinect<< -0.711467261365657,-0.700027989887306,-0.0614422442558011,747.043142353411,
0.673136663054501,-0.707271591245916,-0.215995206123475,814.267472628439,
-0.0778890939478870,0.0921316821623485,-0.992695845758364,1397.51940444262,
0,0,0,1;*/
mat_kinect<<-0.712516170695290,-0.711590631431722,-0.0670606021194355,785.277650742455,
0.667943381743070,-0.719800830594292,-0.212695746272842,830.041069022984,
-0.0874702442707620,0.0649557124833106,-1.00876803037326,1615.62568031354,
0,0,0,1;//0.3323
  pMOG = new BackgroundSubtractorMOG();
  ros::Rate loop_rate(100);
  image_sub_ = it_.subscribe("/camera/rgb/image_raw", 1, imageCb); 			                                    
ros::ServiceServer service = nh_.advertiseService("/srvPoseDepth",getsrvPose);
 object_pcl = nh_.subscribe ("/camera/depth_registered/points", 1, cloud_cb);

  ros::spinOnce();
  int count = 0;
  while (ros::ok())
  {
    ros::spinOnce();
    loop_rate.sleep();

    ++count;
  }
  
  return 0;
}



