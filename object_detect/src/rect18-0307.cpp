#include <ros/ros.h>
#include <image_transport/image_transport.h>
#include <cv_bridge/cv_bridge.h>
#include <sensor_msgs/image_encodings.h>
#include <opencv2/imgproc/imgproc.hpp>
#include <opencv2/highgui/highgui.hpp>
#include <opencv2/opencv.hpp>
#include <opencv2/core/core.hpp>                       //zliu7
#include <opencv2/video/background_segm.hpp>           //zliu7
#include "geometry_msgs/Pose2D.h"
#include <std_msgs/Time.h>
#include <iostream>
#include <stdio.h>
#include <stdlib.h>
#include <geometry_msgs/Twist.h>
#include <vector>
#include <algorithm>
#include <math.h>
#include <object_detect/positionPose.h>
#include "std_msgs/UInt16.h"

using namespace cv;	
using namespace std;
																	//=T
static const std::string OPENCV_WINDOW = "Image window";
const float PI=3.1415926;
float geuler=0,gangel=0;
const float CONST  = -0.84;
 double SIGN(double x) {return (x >= 0.0f) ? +1.0f : -1.0f;}
 double NORM(double a, double b, double c, double d) {return sqrt(a * a + b * b + c * c + d * d);}
int object_shortextent;

int matrix_multiply(double a[][3],double b[][3],double c[][3])
{
	int i = 0, j = 0;

	for(i = 0; i<3; i++)
	{
		for(j=0;j<3;j++)
		   c[i][j]=a[i][0]*b[0][j]+a[i][1]*b[1][j]+a[i][2]*b[2][j];
	
	}
	return 1;
}


int euler2matrix2quaternion(double fi,double si,double pi,double quaternion[4])
{
	double b[3][3]={cos(fi),sin(fi),0,   
			-sin(fi),cos(fi),0,   
			0,0,1};

	double c[3][3]={cos(si),0,-sin(si),
			0,1,0,
			sin(si),0,cos(si)};
	
	double d[3][3]={1,0,0,
			0,cos(pi),sin(pi),
			0,-sin(pi),cos(pi)};

	double matrixP[3][3]={0,0,1,0,1,0,-1,0,0};

	double rm1[3][3],rm[3][3];
	matrix_multiply(d,c,rm1);

	matrix_multiply(rm1,b,rm);/*rm=dcb*/


	matrix_multiply(matrixP,rm,rm1);/*rm1=d*c*b*matrix2*/
	double matrixQ[3][3]={-1,0,0,0,0,1,0,1,0};
	matrix_multiply(rm1,matrixQ,rm);

	ROS_INFO("PdcbQ over\n");

	double r11=rm[0][0],r12=rm[0][1],r13=rm[0][2];
	double r21=rm[1][0],r22=rm[1][1],r23=rm[1][2];
	double r31=rm[2][0],r32=rm[2][1],r33=rm[2][2];

	double q0,q1,q2,q3;

	q0 = ( r11 + r22 + r33 + 1.0f) / 4.0f;
	q1 = ( r11 - r22 - r33 + 1.0f) / 4.0f;
	q2 = (-r11 + r22 - r33 + 1.0f) / 4.0f;
	q3 = (-r11 - r22 + r33 + 1.0f) / 4.0f;
	if(q0 < 0.0f) q0 = 0.0f;
	if(q1 < 0.0f) q1 = 0.0f;
	if(q2 < 0.0f) q2 = 0.0f;
	if(q3 < 0.0f) q3 = 0.0f;
	q0 = sqrt(q0);
	q1 = sqrt(q1);
	q2 = sqrt(q2);
	q3 = sqrt(q3);
	if(q0 >= q1 && q0 >= q2 && q0 >= q3) {
	    q0 *= +1.0f;
	    q1 *= SIGN(r32 - r23);
	    q2 *= SIGN(r13 - r31);
	    q3 *= SIGN(r21 - r12);
	} else if(q1 >= q0 && q1 >= q2 && q1 >= q3) {
	    q0 *= SIGN(r32 - r23);
	    q1 *= +1.0f;
	    q2 *= SIGN(r21 + r12);
	    q3 *= SIGN(r13 + r31);
	} else if(q2 >= q0 && q2 >= q1 && q2 >= q3) {
	    q0 *= SIGN(r13 - r31);
	    q1 *= SIGN(r21 + r12);
	    q2 *= +1.0f;
	    q3 *= SIGN(r32 + r23);
	} else if(q3 >= q0 && q3 >= q1 && q3 >= q2) {
	    q0 *= SIGN(r21 - r12);
	    q1 *= SIGN(r31 + r13);
	    q2 *= SIGN(r32 + r23);
	    q3 *= +1.0f;
	} else {
	    ROS_INFO("coding error\n");
	}
	double r = NORM(q0, q1, q2, q3);
	q0 /= r;
	q1 /= r;
	q2 /= r;
	q3 /= r;


	quaternion[0]=q0;quaternion[1]=q1;quaternion[2]=q2;quaternion[3]=q3;
	/*original order*/
	/*quaternion[0]=q0;quaternion[1]=q1;quaternion[2]=q2;quaternion[3]=q3;*/
	/*right order*/	
	quaternion[0]=q1;quaternion[1]=q0;quaternion[2]=q3;quaternion[3]=q2;
	return 0;
}


//======================================================================================================================================================
//======================================================================================================================================================
geometry_msgs::Pose2D position;				// create a struct to publish cordinate info later on
ros::Publisher pub;
////////////////definition of background substraction//////////////

 Mat fgMaskMOG; //fg mask generated by MOG method
 Ptr <BackgroundSubtractor> pMOG; //MOG Background subtractor

////////////////////////////////////////////////////////////////////
int iLowH_1 = 0;
int iHighH_1 = 69;
int iLowS_1 = 43;
int iHighS_1 = 140;
int iLowV_1 = 126;
int iHighV_1 = 237;


//default capture width and height
const int FRAME_WIDTH = 1288;
const int FRAME_HEIGHT = 964;
//max number of objects to be detected in frame
const int MAX_NUM_OBJECTS=50;
//minimum and maximum object area
const int MIN_OBJECT_AREA = 2*2;
const int MAX_OBJECT_AREA = FRAME_HEIGHT*FRAME_WIDTH/1.5;
//names that will appear at the top of each window
const string windowName = "Original Image";
const string windowName2 = "Thresholded Image 1";

const string windowName_g = "Gaussian Image 1";

const string trackbarWindowName = "Trackbars";

Mat DistortedImg;											//storage for copy of the image raw
Mat	UndistortedImg;											//


double cameraM[3][3] = {{ 521.906925, 0, 330.571743}, {0, 522.912033, 267.759287}, {0, 0, 1}} ;                            
Mat cameraMatrix = Mat(3, 3, CV_64FC1, cameraM);//.inv();

double distortionC[5] = {0.160792,-0.279956,-0.000324,-0.000310, 0};				//distortioncoefficient to be edited
Mat distCoeffs = Mat(1, 5, CV_64FC1, distortionC);							

double rArray[3][3] = {{1, 0, 0}, {0, 1, 0}, {0, 0, 1}};
Mat RArray = Mat(3, 3, CV_64FC1, rArray);					//originally CV_64F

double newCameraM[3][3] = {{532.206543, 0, 330.202770}, {0, 533.930420, 267.077297}, {0, 0, 1}};
Mat NewCameraMatrix = Mat(3, 3, CV_64FC1, newCameraM);
Size UndistortedSize(640, 360);

Mat map1;
Mat map2;												

int xa,ya;
int xa_old, ya_old;
int x_offset=20;               
int y_offset=0;                 


double x_init, z_init, x_final, z_final;
double alpha, beta, angle_dif;

double distance_x, distance_z, rotation_y, asymptote;
int n = 0;
int t= 0;


void createTrackbars(){
	//create window for trackbars

    namedWindow("Control_1", CV_WINDOW_AUTOSIZE);
	//create memory to store trackbar name on window
	//create trackbars and insert them into window
	//3 parameters are: the address of the variable that is changing when the trackbar is moved(eg.H_LOW),
	//the max value the trackbar can move (eg. H_HIGH), 
	//and the function that is called whenever the trackbar is moved(eg. on_trackbar)
	//                                  ---->    ---->     ---->      
 //Create trackbars in "Control" window
	cvCreateTrackbar("LowH_1", "Control_1", &iLowH_1, 179); //Hue (0 - 179)
	cvCreateTrackbar("HighH_1", "Control_1", &iHighH_1, 179);

	cvCreateTrackbar("LowS_1", "Control_1", &iLowS_1, 255); //Saturation (0 - 255)
	cvCreateTrackbar("HighS_1", "Control_1", &iHighS_1, 255);

	cvCreateTrackbar("LowV_1", "Control_1", &iLowV_1, 255); //Value (0 - 255)
	cvCreateTrackbar("HighV_1", "Control_1", &iHighV_1, 255);
	
}
string intToString(int number)
{
	std::stringstream ss;
	ss << number;
	return ss.str();
}


void drawObject(int x, int y,Mat &frame){  //RED TARGET

	//use some of the openCV drawing functions to draw crosshairs
	//on the tracked image!

	circle(frame,Point(x,y),20,Scalar(0,255,0),2);

    if(y-25>0)
    line(frame,Point(x,y),Point(x,y-25),Scalar(0,255,0),2);
    else line(frame,Point(x,y),Point(x,0),Scalar(0,255,0),2);
    if(y+25<FRAME_HEIGHT)
    line(frame,Point(x,y),Point(x,y+25),Scalar(0,255,0),2);
    else line(frame,Point(x,y),Point(x,FRAME_HEIGHT),Scalar(0,255,0),2);
    if(x-25>0)
    line(frame,Point(x,y),Point(x-25,y),Scalar(0,255,0),2);
    else line(frame,Point(x,y),Point(0,y),Scalar(0,255,0),2);
    if(x+25<FRAME_WIDTH)
    line(frame,Point(x,y),Point(x+25,y),Scalar(0,255,0),2);
    else line(frame,Point(x,y),Point(FRAME_WIDTH,y),Scalar(0,255,0),2);

	putText(frame,intToString(x)+","+intToString(y),Point(x,y+30),1,1,Scalar(0,255,0),2);

}

void morphOps(Mat &thresh)
{
	//create structuring element that will be used to "dilate" and "erode" image.
    //dilate with larger element so make sure object is nicely visible

	erode(thresh,thresh,getStructuringElement( MORPH_RECT,Size(2,2)));
	dilate(thresh,thresh,getStructuringElement( MORPH_RECT,Size(2,2)));	
}

bool ojbtarget = false;

void trackFilteredObject1(int &x, int &y, Mat threshold, Mat &cameraFeed, int &x_offset, int &y_offset)
{
	ROS_INFO("trackFilteredObject1");

	Mat temp;
	threshold.copyTo(temp);				//these two vectors needed for output of findContours
	
	vector< vector<Point> > contours;
	vector<Vec4i> hierarchy;			//find contours of filtered image using openCV findContours function
	
	findContours(temp,contours,hierarchy,CV_RETR_EXTERNAL,CV_CHAIN_APPROX_SIMPLE );
	
	double refArea = 0;
	bool objectFound = false;
	ojbtarget = false;
	if (hierarchy.size() > 0) 			//if number of objects greater than MAX_NUM_OBJECTS we have a noisy filter
	{
		int numObjects = hierarchy.size();
		
		if(numObjects<MAX_NUM_OBJECTS)
		{
			for (int index = 0; index >= 0; index = hierarchy[index][0]) 
			{
				Moments moment = moments((cv::Mat)contours[index]);	//use moments method to find our filtered object
				double area = moment.m00;

				//if the area is less than 20 px by 20px then it is probably just noise
				//if the area is the same as the 3/2 of the image size, probably just a bad filter
				//we only want the object with the largest area so we safe a reference area each
				//iteration and compare it to the area in the next iteration.
				//if(area>MIN_OBJECT_AREA && area<MAX_OBJECT_AREA && area>refArea)
				if(area>MIN_OBJECT_AREA && area>refArea)
				{
					x = moment.m10/area;
					y = moment.m01/area;
					ojbtarget=true;
					ROS_INFO(" IMAGE DETECTED!");
					refArea = area;
			
					if (n == 0)
					{
					xa_old = x;
					ya_old = y;
					n = 1;
					}
			
					if ((abs(xa_old-x)<= 2) && (abs(ya_old-y) <= 2))
					{
						xa = xa_old;
						ya = ya_old;
					}
					else 
					{	
					    xa = x+x_offset;
					    ya = y+y_offset;
					    xa_old = x+x_offset;
					    ya_old = y+y_offset;				    
				        }
				}
				else {ROS_INFO("no thing!");ojbtarget=false;}

			}
			//let user know you found an object
			//if(objectFound == true)
			//{
			//putText(cameraFeed,"Tracking Object",Point(0,50),2,1,Scalar(0,255,0),2);
				//draw object location on screen
			//drawObject(xa,ya,cameraFeed);

		}

		else //putText(cameraFeed,"TOO MUCH NOISE! ADJUST FILTER",Point(0,50),1,2,Scalar(0,0,255),2);
		{ROS_INFO("no thing!");ojbtarget=false;}
	}
ROS_INFO("obj detected?%d",ojbtarget);
}


//======================================================================================================================================================
geometry_msgs::Twist cv_p;
geometry_msgs::Twist pose;

bool getPositionPose(object_detect::positionPose::Request &req,object_detect::positionPose::Response &res)
{
	//if(req.flag&&ojbtarget)
	if(req.flag)
	{
		res.cv_p = cv_p;
		res.pose = pose;
		return true;
	}
	return false;
} 
  void imageCb(const sensor_msgs::ImageConstPtr& msg)								  //callback function defination
  {
     
    cv_bridge::CvImagePtr cv_ptr;													   
    try
    {
      cv_ptr = cv_bridge::toCvCopy(msg, sensor_msgs::image_encodings::BGR8);   			//convert ROS image to CV image and make copy of it storing in cv_ptr(a pointer)
    }
    catch (cv_bridge::Exception& e)
    {
      ROS_ERROR("cv_bridge exception: %s", e.what());
      return;
    }

	/*	image working procedure starting from here inside the main function.
	 *  The purpose of the image processing is to use the existing video to working out the 
	 *  cordinate of the detected object, using color extraction technique.
	 */
	
    bool trackObjects = true;
    bool useMorphOps = true;

	Mat cameraFeed;

	Mat HSV_1;
    
	Mat threshold_1;
	//x and y values for the location of the object
	int x=0, y=0;
	createTrackbars();

		//store image to matrix
		cv_ptr->image.copyTo(DistortedImg);											//=Tan= copy the image from ardrone to DistortedImg for processing
		initUndistortRectifyMap(cameraMatrix, distCoeffs, RArray, NewCameraMatrix, UndistortedSize, CV_32FC1, map1, map2);
		remap(DistortedImg, cameraFeed, map1, map2, INTER_LINEAR, BORDER_CONSTANT, Scalar(0,0,0));		// maybe try this one instead of the above two instead: cv::undistort(image, undistorted, cameraMatrix, distCoeffs);
//////////////////////////////////////////background substraction 1////////////////////////////////////////////////////////////////
  pMOG->operator()(cameraFeed, fgMaskMOG);        
             imshow("cameraFeed", cameraFeed);
           Mat pFrame=fgMaskMOG;
           Mat element1 = getStructuringElement(0, Size(20,20), Point(-1,-1));
		morphologyEx(pFrame, pFrame, MORPH_CLOSE, element1);//闭运算
		imshow("CLOSE", pFrame);
	   vector<vector<Point> > contours;
	   vector<Vec4i> hierarchy;			//find contours of filtered image using openCV findContours function
	

	        double quaternion[4];
		findContours(pFrame, contours, hierarchy, CV_RETR_EXTERNAL, CV_CHAIN_APPROX_SIMPLE);  //ŒìË÷ÂÖÀª

		Mat result(pFrame.size(), CV_8U, Scalar(255));
		drawContours(result, contours, -1, Scalar(0), 2);// draw all contours//
		imshow("4.All Contours", result);

		// Eliminate too short or too long contours  
		int cmin = 30;  // minimum contour length  //Change these two parameters
		int cmax = 500; // maximum contour length  
		vector<vector<Point> >::const_iterator itc = contours.begin();
		vector<vector<Point> > filtercontours;
		while (itc != contours.end())
		{

			if (itc->size() >= cmin && itc->size() <= cmax)
				filtercontours.push_back(*itc);
			++itc;
		}

		// Let's now draw black contours on white image  
		result.setTo(Scalar(255));
		drawContours(result, filtercontours, -1, Scalar(0), 1);  // draw all contours £¬in black£¬with a thickness of 1    
		imshow("5.Standard Contours", result);
		cout << "A New Image" << endl;
		vector<vector<Point> >::const_iterator itcon = filtercontours.begin();
		float angl;
		for (; itcon != filtercontours.end(); ++itcon)
		{
			RotatedRect rRect = minAreaRect(Mat(*itcon));
			Point2f vertices[4];
			rRect.points(vertices);
			for (int i = 0; i < 4; ++i)
				line(result, vertices[i], vertices[(i + 1) % 4], CV_RGB(255, 0, 0), 2, 8, 0);//»­×îÐ¡Íâ°üŸØÐÎ
                        //publisher the object width
			ROS_INFO("Width%d,height%d",(int)rRect.size.width,(int)rRect.size.height);
			/*ros::Time start_time=ros::Time::now();
			ros::publisher widthval_pub=n.advertise<std_msgs::UInt16>("widthval",1000);
  			ros::Rate loop_rate(10);
			while (ros::Time::now()-start_time<ros::Duration(5))
 			 {
			  widthval=(int)rRect.size.width;
  			  std_msgs::UInt16 msg;
			  msg.data=widthval;
    		          value_pub.publish(msg);
   			  ros::spinOnce();
   			  loop_rate.sleep();   		
  			 }*/
			

			if(rRect.size.width> rRect.size.height)
			{
				
				object_shortextent = (int)rRect.size.height;
				ROS_INFO("calculated length of object %d",object_shortextent);
			        Point2f pttA[2];
				pttA[0].x = (vertices[0].x + vertices[1].x) / 2;
				pttA[0].y = (vertices[0].y + vertices[1].y) / 2;
				pttA[1].x = (vertices[2].x + vertices[3].x) / 2;
				pttA[1].y = (vertices[2].y + vertices[3].y) / 2;
				Point2f pttCent;
				pttCent.x = (pttA[0].x + pttA[1].x) / 2;
				pttCent.y = (pttA[0].y + pttA[1].y) / 2;
				line(result, pttA[0], pttA[1], CV_RGB(255, 0, 0), 2, 8, 0);
				circle(result, pttCent, 2, Scalar(0), 3);
			 	angl = atan2((pttA[1].y - pttA[0].y), (pttA[1].x - pttA[0].x));
				
				char buf[20];
				
				//putText(result, buf, rRect.center, CV_FONT_HERSHEY_COMPLEX, 1, Scalar(0, 0, 0));

			}
			else
			{
				object_shortextent = (int)rRect.size.width;
				ROS_INFO("calculated length of object %d",object_shortextent);
				Point2f ptt[2];
				ptt[0].x = (vertices[1].x + vertices[2].x) / 2;
				ptt[0].y = (vertices[1].y + vertices[2].y) / 2;
				ptt[1].x = (vertices[0].x + vertices[3].x) / 2;
				ptt[1].y = (vertices[0].y + vertices[3].y) / 2;
				Point2f pttCen;
				pttCen.x = (ptt[0].x + ptt[1].x) / 2;
				pttCen.y = (ptt[0].y + ptt[1].y) / 2;
				line(result, ptt[0], ptt[1], CV_RGB(255, 0, 0), 2, 8, 0);
				circle(result, pttCen, 2, Scalar(0), 3);
				 angl = atan2((ptt[1].y - ptt[0].y), (ptt[1].x - ptt[0].x));
			
				char buf1[20];
				
				//putText(result, buf1, rRect.center, CV_FONT_HERSHEY_COMPLEX, 1, Scalar(0, 0, 0));
			}
			gangel = angl;
			ROS_INFO( "before angl:%f rad %f", gangel,angl * 180 / CV_PI);
			if (angl<0)
				angl=-angl;
			else angl = PI-angl;
			gangel = angl;
			ROS_INFO( "after angl:%f rad %f", gangel,angl * 180 / CV_PI);
			double euler=(double)(CONST+angl);

			euler2matrix2quaternion(PI,0,euler,quaternion);
			ROS_INFO("euler:%f\n",euler);
			geuler=euler;
			ROS_INFO("quaternion:%f,%f,%f,%f\n",quaternion[0],quaternion[1],quaternion[2],quaternion[3]);
			imshow("6 Final Result", result);
		}

               Mat fgMaskMOG_ROI(fgMaskMOG,Rect(20, 0, 610, 190));
		//perform morphological operations on thresholded image to eliminate noise
		//and emphasize the filtered object(s)
		if(useMorphOps)
		{
			//morphOps(threshold_1);
                        //morphOps(fgMaskMOG);
                        morphOps(fgMaskMOG_ROI);
		}
		//pass in thresholded frame to our object tracking function
		//this function will return the x and y coordinates of the
		//filtered object
		if(trackObjects)
		{

                        trackFilteredObject1(x,y,fgMaskMOG_ROI,cameraFeed,x_offset,y_offset);
		}


//		imshow(windowName2,threshold_1);
		imshow(windowName,cameraFeed);		
                imshow(windowName_g,fgMaskMOG);

 // imwrite( "fgMaskMOG.jpg", fgMaskMOG);
////////////////////////////////////////HSV/////////////////////////////////////////////////////////////////////////////////////
		//convert frame from BGR to HSV colorspace
		cvtColor(cameraFeed,HSV_1,COLOR_BGR2HSV);

		//fiter HSV image between values and store filtered image to
		//threshold matrix
				
		//output the after-threshold matrix to Mat threshold
		inRange(HSV_1,Scalar(iLowH_1, iLowS_1, iLowV_1),Scalar(iHighH_1, iHighS_1, iHighV_1),threshold_1); 
//////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////



//		imshow(windowName2,threshold_1);
		imshow(windowName,cameraFeed);		
                imshow(windowName_g,fgMaskMOG);
//               imwrite("table.jpg", cameraFeed);
//		imshow("HSV",HSV_1);
  cv::waitKey(30);  //to show video steady.
////////////////////////////////////publish object information to path planner///////////////////////////////////////////////
  ros::NodeHandle nh;
  //geometry_msgs::Twist cv_p;
  cv_p.linear.x=xa;
  cv_p.linear.y=ya;
  cv_p.linear.z=0;
  cv_p.angular.x=0;
  cv_p.angular.y=0;
  cv_p.angular.z=0;
//geometry_msgs::Twist pose;
  pose.linear.x = quaternion[0];
  pose.linear.y = quaternion[1];
  pose.linear.z = quaternion[2];
  pose.angular.x= quaternion[3];
  pose.angular.y=0;
  pose.angular.z=0;

  std_msgs::UInt16 value;
  value.data = object_shortextent;

  ROS_INFO("Start to publish");
  ros::Publisher pose_pub = nh.advertise<geometry_msgs::Twist>("transformer_pose",1);
  //publisher the short extent length
  ros::Publisher shortextent_pub = nh.advertise<std_msgs::UInt16>("short_extent",1);
  ros::Publisher cv_p_pub = nh.advertise<geometry_msgs::Twist>("cv_position",1);
  
  //ROS_INFO("target.x rect 537 zym is %.2f, target.y is %.2f)",cv_p.linear.x,cv_p.linear.y);
  //ROS_INFO("angel:%fRAD,%f du,euler:%f\n",gangel,gangel*180/PI,geuler);
  //ROS_INFO("quaternion:%f,%f,%f,%f\n",quaternion[0],quaternion[1],quaternion[2],quaternion[3]);
  std_msgs::Time time;
  time.data=ros::Time::now();

  while(ros::Time::now() - time.data < ros::Duration(0.05))
  {
     pose_pub.publish(pose);
     //sleep(1);
     shortextent_pub.publish(value);
     cv_p_pub.publish(cv_p);

    
  }
////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

    
  }

int main(int argc, char** argv)
{
  ros::init(argc, argv, "object_detect");
  ros::NodeHandle nh_;
  image_transport::ImageTransport it_(nh_);
  image_transport::Subscriber image_sub_;
  
  
  pMOG = new BackgroundSubtractorMOG();
  ros::Rate loop_rate(100);
  image_sub_ = it_.subscribe("/camera/rgb/image_raw", 1, imageCb); 			                                    
ros::ServiceServer service = nh_.advertiseService("/srvPositionPose",getPositionPose);
  ros::spinOnce();
  int count = 0;
  while (ros::ok())
  {
    ros::spinOnce();
    loop_rate.sleep();

    ++count;
  }
  
  return 0;
}



